{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:17.585366Z",
     "start_time": "2025-05-28T18:22:17.575912Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from models.DiffLoad.diffusion.layers import CondModel_v2\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.helper import make_beta_schedule, EMA, ObjectView\n",
    "from utils.plots import hdr_plot_style\n",
    "hdr_plot_style()\n",
    "from tqdm import tqdm \n",
    "from models.DiffLoad.ddpm import DDPM1d\n",
    "from utils.config import config_dataset, config_ddpm, config_nn"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:17.598417Z",
     "start_time": "2025-05-28T18:22:17.594897Z"
    }
   },
   "source": "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:17.634228Z",
     "start_time": "2025-05-28T18:22:17.627405Z"
    }
   },
   "source": [
    "configs = {\n",
    "    'epoch': 80000,\n",
    "    'batch_size': 3000,\n",
    "    'learning_rate': 1e-4,\n",
    "    'lr_decay': 0.99,\n",
    "    'lr_decay_step': 200,\n",
    "    'mode': None, # [\"checkpoint\", None]\n",
    "    'use_MLP': True,\n",
    "    'use_solar':True\n",
    "}\n",
    "configs.update(config_dataset)\n",
    "configs.update(config_ddpm)\n",
    "configs.update(config_nn)\n",
    "args = ObjectView(configs)\n",
    "args.save_name = \"diff_phy\" if args.use_solar else \"diff_base\"\n",
    "print(args.save_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_phy\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:17.692943Z",
     "start_time": "2025-05-28T18:22:17.667393Z"
    }
   },
   "source": [
    "X_train = torch.load(\"../data/Pecan Street Smart Meter Data (large) (tensor)/X_train.pt\")\n",
    "X_test = torch.load(\"../data/Pecan Street Smart Meter Data (large) (tensor)/X_test.pt\")\n",
    "cond_train = torch.load(\"../data/Pecan Street Smart Meter Data (large) (tensor)/cond_train.pt\")\n",
    "cond_test = torch.load(\"../data/Pecan Street Smart Meter Data (large) (tensor)/cond_test.pt\")\n",
    "PV_base_train = torch.load(\"../data/Pecan Street Smart Meter Data (large) (tensor)/PV_base_train.pt\")\n",
    "PV_base_test = torch.load(\"../data/Pecan Street Smart Meter Data (large) (tensor)/PV_base_test.pt\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:17.703486Z",
     "start_time": "2025-05-28T18:22:17.698007Z"
    }
   },
   "source": [
    "X_train.min()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:17.725955Z",
     "start_time": "2025-05-28T18:22:17.721463Z"
    }
   },
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8760, 96])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T18:22:40.641213Z",
     "start_time": "2025-05-28T18:22:17.758670Z"
    }
   },
   "source": [
    "# Select betas\n",
    "n_steps = args.n_steps\n",
    "\n",
    "betas = make_beta_schedule(schedule='linear', n_timesteps=n_steps, start=args.beta_start, end=args.beta_end)\n",
    "betas = betas.to(device)\n",
    "model = CondModel_v2(args)\n",
    "if args.mode == \"checkpoint\":\n",
    "    print(f\"load chechpoint\")\n",
    "    checkpoint = torch.load(f'../result/pecan/models/{args.save_name}_80000.pth')\n",
    "    model = checkpoint['ddpm'].model\n",
    "    args.learning_rate = args.learning_rate * args.lr_decay ** (args.epoch/args.lr_decay_step)\n",
    "\n",
    "model = model.to(device)\n",
    "X_train = X_train.to(device)\n",
    "cond_train = cond_train.to(device)\n",
    "PV_base_train = PV_base_train.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step, gamma=args.lr_decay)\n",
    "ddpm = DDPM1d(model, betas, n_steps, (args.input_dim,), loss_type='l2')\n",
    "# Create EMA model\n",
    "ema = EMA(args.ema_decay)\n",
    "ema.register(model)\n",
    "\n",
    "Loss = []\n",
    "for j in tqdm(range(args.epoch)):\n",
    "    # X is a torch Variable\n",
    "    permutation = torch.randperm(X_train.size()[0])\n",
    "    for i in range(0, X_train.size()[0], args.batch_size):\n",
    "        # Retrieve current batch \n",
    "        indices = permutation[i:i+args.batch_size]\n",
    "        batch_x = X_train[indices]\n",
    "        batch_x = batch_x + 0.05 * torch.randn_like(batch_x)\n",
    "        batch_cond = cond_train[indices]\n",
    "        # Compute the loss.\n",
    "        if args.use_solar == True:\n",
    "            batch_PV_base = PV_base_train[indices]\n",
    "            loss = ddpm(batch_x, batch_cond, batch_PV_base)\n",
    "        else:\n",
    "            loss = ddpm(batch_x, batch_cond)\n",
    "        # Before the backward pass, zero all of the network gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass: compute gradient of the loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Perform gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        # Calling the step function to update the parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # Update the exponential moving average\n",
    "        ema.update(model)\n",
    "        ddpm.model = model\n",
    "    if (j+1) % 100 == 0:\n",
    "        Loss.append(loss.item())\n",
    "    if (j+1) % 10000 == 0:\n",
    "        print(\"loss: \", loss.item())\n",
    "    if (j+1) % 10000 == 0 or (j+1) == args.epoch:\n",
    "        checkpoint = {\n",
    "            'config': configs,\n",
    "            'ddpm': ddpm,\n",
    "            'Loss': Loss\n",
    "        }\n",
    "        torch.save(checkpoint, f\"../result/models/pecan/{args.save_name}_{j+1}.pth\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cybsbbbb/anaconda3/envs/diff_load/lib/python3.10/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/80000 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 43\u001B[0m\n\u001B[1;32m     41\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Backward pass: compute gradient of the loss with respect to parameters\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Perform gradient clipping\u001B[39;00m\n\u001B[1;32m     45\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1.\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/diff_load/lib/python3.10/site-packages/torch/_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    647\u001B[0m     )\n\u001B[0;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/diff_load/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/diff_load/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    825\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot(Loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cond_test = cond_test.to(device)\n",
    "PV_base_test = PV_base_test.to(device)\n",
    "X_test_hat = ddpm.sample_seq(batch_size=len(cond_test), cond=cond_test, PV_base=PV_base_test)[-1]\n",
    "X_test_hat = X_test_hat.to(\"cpu\")\n",
    "X_test_hat = X_test_hat.reshape(args.num_class, -1, 96)\n",
    "X_test = X_test.reshape(args.num_class, -1, 96)\n",
    "for j in range(config_dataset[\"num_class\"])[:10]:\n",
    "    plt.figure(figsize=(36,6), dpi=300)\n",
    "    plt.subplot(1,4,1)\n",
    "    for i in range(len(X_test[j])):\n",
    "        plt.plot(X_test[j][i])\n",
    "    plt.title(\"actual data\")\n",
    "    plt.subplot(1,4,2)\n",
    "    for i in range(len(X_test_hat[j])):\n",
    "        plt.plot(X_test_hat[j][i])\n",
    "    plt.title(\"generated load profile (MLP0_Solar0)\")\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.plot(X_test_hat[j].mean(dim=0), label = \"mean of generated data\")\n",
    "    plt.plot(X_test[j].mean(dim=0), label = \"mean of actual data\")\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.plot(X_test_hat[j].var(dim=0), label = \"var of generated data\")\n",
    "    plt.plot(X_test[j].var(dim=0), label = \"var of actual data\")\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
